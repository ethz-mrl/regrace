# REGRACE: A Robust and Efficient Graph-based Re-localization Algorithm using Consistency Evaluation

A novel approach that addresses these challenges of scalability and perspective difference in re-localization by using LiDAR-based submaps. 

[![arXiv](https://img.shields.io/badge/arXiv-2503.03599-b31b1b.svg)](https://arxiv.org/abs/2503.03599)
[![GitHub License](https://img.shields.io/github/license/smartroboticslab/regrace?label=License&color=%23e11d48&cacheSeconds=3600)
](https://github.com/tum-esm/pyra/blob/main/LICENSE.md)
[![](https://img.shields.io/github/v/tag/smartroboticslab/regrace?label=Latest%20Release&color=%23e11d48&cacheSeconds=60)
](https://github.com/smartroboticslab/regrace/releases)

## ðŸª› Installation

1) Create a virtual environment
```
python3 -m venv .venv
```

2) One can install the dependencies using pip or pdm

```bash
pip install -r requirements.txt
# or (choose one)
pdm install
```

3) Compile and install the `pointnet2` package. Please follow the instructions in the [`pointnet2-wheel` folder](packages/pointnet2-wheel/README.md) to compile a wheel and install it.

4) For good practices, export the following CUDA seed variables to your `~/.bashrc`

```bash
export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512'
export 'CUBLAS_WORKSPACE_CONFIG=:4096:8'
```

## ðŸ”— Download data and weights

1) Download the [SemanticKITTI dataset](https://semantic-kitti.org/). 

2) Download Cylinder3D weight from [here](https://drive.usercontent.google.com/download?id=1q4u3LlQXz89LqYW3orXL5oTs_4R2eS8P&export=download&authuser=0) and save it to `./config/cyl3d_weights.pt`

3) If you want to use the pretrained model, download the weights trained on KITTI sequences 00 to 10 from [our latest release](https://github.com/smartroboticslab/regrace/releases) and save it to `./config/weights.pt`. Further instructions on how to use the weights are provided in the [Testing](#-testing) section.

## ðŸ—‚ï¸ Generate submaps

> ðŸš¨ _A future release will target offering the option to generate the triplets and the parquet files in one step only._ ðŸš¨

First, adjust the parameters in the configuration YAML [`data-generation.yaml`](config/data-generation.yaml):

1) `sequence` to the desired KITTI sequence
2) `kitti_dir` to the root path of the SemanticKITTI dataset
3) `output_folder` to the desired output folder

Then, run the following command:

```bash
python run.py --config_file config/data-generation.yaml --generate_submaps
```

This will create a folder in the following structure:

```
preprocessed_data_folder
â”œâ”€â”€ seq00
â”‚   â”œâ”€â”€ single-scan
â”‚   |   â”œâ”€â”€ label-prediction
|   |   â””â”€â”€ probability-labels
â”‚   â””â”€â”€ submap
â”‚       â”œâ”€â”€ all-points
|       â””â”€â”€ cluster
â”œâ”€â”€ seq01
...
```

The `single-scan` folder contains the predictions of the Cylinder3D model for each scan in the sequence. The `submap` folder contains the submaps generated by accumulating scans. Those submaps are already voxelized (`all-points`) and clustered (`cluster`). The total size for KITTI sequence 00 to 10 is around 1.5TB. If you don't have enough memory, you can intercalate each sequence generation with the generation of the triplets below, deleting the `all-points` and `cluster` folders after the triplets are generated. Further steps will be explained at [the end of this section](#-saving-memory-while-generating-submaps).

We then compact the submaps into a parquet file containing the each point of the valid clusters and the respective normal vector. Each parquet is accompanied by a pickle file containing the metadata of the submap, such as position and positive maps. We decided to split the generation in two steps to allow debugging without the need to generate the submaps again. To generate the parquet and pickle files, adjust the parameters in the configuration YAML [`default.yaml`](config/default.yaml):

1) `dataset/train_folders` and `dataset/test_folders` to the folders of the preprocessed data for each KITTI sequence (`.../submap/cluster`). You can add multiple folders as a list.
2) `dataset/preprocessing_folder` to the folder where the compressed preprocessed data should be stored
3) `flag/generate_triplets` to `True`
4) `flag/train` and `flag/test` to `False`

Then, run the following command:

```bash
python run.py --config_file <YOUR_CONFIG>.yaml
```

This will create a folder in the following structure:

```
preprocessing_folder
â”œâ”€â”€ 00
â”‚   â”œâ”€â”€ pickle
â”‚   â””â”€â”€ parquet
â”œâ”€â”€ 01
â”‚   â”œâ”€â”€ pickle
â”‚   â””â”€â”€ parquet
...
```

and a folder `\data\pickle_list\eval_seqXX` containing the compacted dataset for faster loading during training and testing.

#### ðŸ’¡ Saving memory while generating submaps

1) Generate the submaps for the first sequence
2) If you use the sequence as test and train folder, a error will raise that you can't use the same folder for both. In this case, you can create a symbolic link to the folder mimicking another sequence. For example, if you generated the submaps for sequence 00, you can create a symbolic link to the folder `00` named `01` and use it as the test folder.
3) Generate the triplets for the first sequence, and cancel the process after the training triplets are generated.
4) Delete the `all-points` and `cluster` folders for the first sequence.
5) Repeat the process for the next sequence.



### ðŸ“Š Testing

To test the model, you need to have the model trained. Weights are available in the [latest release](https://github.com/smartroboticslab/regrace/releases). Adjust the in the configuration YAML [`default.yaml`](config/default.yaml) as:

1) `flag/train` to `False` and `flag/test` to `True`. 
2) Add the path to the weights in `training/checkpoint_path`.
3) Set `flags/initialize_from_checkpoint` to `True`.
4) `dataset/preprocessing_folder` to the compressed preprocessed data folder.
5) `flag/generate_triplets` to `False`.

Then, run the following command:

```bash
python run.py --config_file <YOUR_CONFIG>.yaml
```

Your output will be a table with the metrics for the test set. 

### ðŸš€ Training

To train the model, you need to adjust the in the configuration YAML [`default.yaml`](config/default.yaml) as:

1) `dataset/preprocessing_folder` to the compressed preprocessed data folder.
2) `flag/generate_triplets` to `False`.
3) `flag/train` to `True` and `flag/test` to `False`.
4) `flag/initialize_from_checkpoint` to `False`.
5) `flag/generate_triplets` to `False`.

Then, run the following command:

```bash
python run.py --config_file <YOUR_CONFIG>.yaml
```

If you want to use [wandb](https://wandb.ai/) to log the training, you can set the `wandb_logging` flag in the configuration YAML to `True` and set the `project` and `entity` in [`utils.py`](src/reloc_gnn/utils/utils.py) to your desired project and entity (usually your username). Don't forget to login first:
    
```bash
wandb login
```

For the final refinement step, set the configuration YAML [`default.yaml`](config/default.yaml) as:

```yaml
training:
  batch_size: 90
  checkpoint_path: <path_to_checkpoint>
  epochs: 50
  loss:
    margin: 1.0
    p: 2
    type: both
  num_workers: 12
  optimizer:
    lr: 1.0e-05
  scheduler:
    decay_rate: 0.1
    milestones:
    - 25
```

Also set `flags/initialize_from_checkpoint` to `True`. Then, run the following command:

```bash
python run.py --config_file <YOUR_CONFIG>.yaml
```